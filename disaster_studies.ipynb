{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1290b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.initializers import Constant\n",
    "from BetterRnnlm import BetterRnnlm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from SGD import SGD\n",
    "from RnnlmTrainer import RnnlmTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1e4126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv('../dataset/disaster/nlp_getting_started/train.csv')\n",
    "test=pd.read_csv('../dataset/disaster/nlp_getting_started/test.csv')\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e6504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([tweet,test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef20ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341baef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03b4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cae961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77723690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d52fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961e6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958500f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in df['text']:\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a9da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd59154b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['our', 'deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us'],\n",
       " ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada'],\n",
       " ['all',\n",
       "  'residents',\n",
       "  'asked',\n",
       "  'shelter',\n",
       "  'place',\n",
       "  'notified',\n",
       "  'officers',\n",
       "  'no',\n",
       "  'evacuation',\n",
       "  'shelter',\n",
       "  'place',\n",
       "  'orders',\n",
       "  'expected'],\n",
       " ['people', 'receive', 'wildfires', 'evacuation', 'orders', 'california'],\n",
       " ['just',\n",
       "  'got',\n",
       "  'sent',\n",
       "  'photo',\n",
       "  'ruby',\n",
       "  'alaska',\n",
       "  'smoke',\n",
       "  'wildfires',\n",
       "  'pours',\n",
       "  'school']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5de689",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Disaster_Tweet/model/tokenize_model/tokenizer_obj.pkl', 'rb') as file:\n",
    "    tokenizer_obj = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7978b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer_obj.word_index\n",
    "batch_size = 20\n",
    "wordvec_size = 50\n",
    "hidden_size = 50\n",
    "time_size = 50\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "vocab_size = len(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc7cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
    "tweet_pad = pad_sequences(sequences, maxlen = time_size, truncating = 'post', padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a819507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504922c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RnnlmTrainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea00b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "text=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3055921",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ppl = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(X_train, y_train, max_epoch = 1, batch_size = batch_size,\n",
    "               time_size = time_size, max_grad = max_grad)\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fb930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5d05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
